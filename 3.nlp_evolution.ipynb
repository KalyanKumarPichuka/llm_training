{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_1. Count Vectorizer (Bag of Words)_**\n",
    "\n",
    "<u>**Concept:**</u>  \n",
    "The simplest NLP technique, where we convert text into a matrix of word occurrences.\n",
    "\n",
    "<u>**Example:**</u>  \n",
    "Consider two simple sentences:  \n",
    "1. \"NLP is evolving fast\"  \n",
    "2. \"Machine learning is evolving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_1 = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"the mat is on the floor\"\n",
    "]\n",
    "\n",
    "corpus_2 = [\n",
    "    \"artificial intelligence is the future\",\n",
    "    \"machine learning is a subset of artificial intelligence\",\n",
    "    \"the future of AI is bright\"\n",
    "]\n",
    "\n",
    "corpus_3 = [\"NLP is evolving fast\", \"Machine learning is evolving\", \"Future of NLP is bright\"]\n",
    "\n",
    "corpus_4 = [\n",
    "    \"Machine learning improves decision-making in businesses.\",\n",
    "    \"Businesses use machine learning for data analysis.\",\n",
    "    \"Deep learning and neural networks outperform traditional models.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['bright' 'evolving' 'fast' 'future' 'is' 'learning' 'machine' 'nlp' 'of']\n",
      "Word Frequency Matrix:\n",
      " [[0 1 1 0 1 0 0 1 0]\n",
      " [0 1 0 0 1 1 1 0 0]\n",
      " [1 0 0 1 1 0 0 1 1]]\n",
      "Cosine Similarity:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Corpus of sentences\n",
    "corpus = corpus_3\n",
    "# Convert text into a matrix of word occurrences\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print results\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())  # Unique words\n",
    "print(\"Word Frequency Matrix:\\n\", X.toarray())  # Word frequency matrix\n",
    "print(\"Cosine Similarity:\\n\", cosine_similarity(X[0],X[1])[0][0])  # Cosine similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Analysis:_**\n",
    "\n",
    "<u>**Analysis:**</u>  \n",
    "- Each row represents a sentence.  \n",
    "- Each column represents a word.  \n",
    "- The numbers show how many times a word appears in each sentence.\n",
    "\n",
    "<u>**Limitations:**</u>  \n",
    "- Does not capture meaning.  \n",
    "- Common words like \"is\" are treated the same as \"NLP\" or \"Machine.\"  \n",
    "- Ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_2. TF-IDF (Term Frequency-Inverse Document Frequency)_**\n",
    "\n",
    "<u>**Concept:**</u>  \n",
    "Improves Count Vectorizer by reducing the importance of commonly occurring words.\n",
    "\n",
    "<u>**Example:**</u>  \n",
    "We apply TF-IDF on the same corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bright' 'evolving' 'fast' 'future' 'is' 'learning' 'machine' 'nlp' 'of']\n",
      "[[0.         0.4804584  0.63174505 0.         0.37311881 0.\n",
      "  0.         0.4804584  0.        ]\n",
      " [0.         0.44451431 0.         0.         0.34520502 0.5844829\n",
      "  0.5844829  0.         0.        ]\n",
      " [0.50461134 0.         0.         0.50461134 0.29803159 0.\n",
      "  0.         0.38376993 0.50461134]]\n",
      "Cosine Similarity:\n",
      " 0.34237311738896226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())  # TF-IDF scores\n",
    "\n",
    "# Cosine similarity using TF-IDF\n",
    "print(\"Cosine Similarity:\\n\", cosine_similarity(X[0],X[1])[0][0])  # Cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding for 'nlp': [ 0.1476101  -0.03066943 -0.09073226  0.13108103 -0.09720321]\n",
      "Words most similar to 'evolving': [('learning', 0.8512495160102844), ('future', 0.774187445640564), ('bright', 0.6178626418113708), ('fast', 0.20796996355056763), ('is', 0.2018294483423233), ('machine', 0.18539191782474518), ('of', 0.07266710698604584), ('nlp', -0.6734715700149536)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Tokenize the corpus (split each sentence into words)\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_corpus, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Get the word embedding for \"nlp\"\n",
    "print(\"Word embedding for 'nlp':\", model.wv[\"nlp\"])\n",
    "\n",
    "# Find words most similar to \"evolving\"\n",
    "print(\"Words most similar to 'evolving':\", model.wv.most_similar(\"evolving\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nlp', 'is', 'evolving', 'fast'],\n",
       " ['machine', 'learning', 'is', 'evolving'],\n",
       " ['future', 'of', 'nlp', 'is', 'bright']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding for 'alice': [ 1.90299869e-01 -2.04534546e-01 -3.73386443e-01 -7.34372884e-02\n",
      "  6.52075931e-02  1.18548110e-01  1.10937469e-01  3.34335774e-01\n",
      " -3.95782024e-01 -2.06006467e-01 -6.18554689e-02  1.98676437e-02\n",
      "  2.47976258e-02  1.40431687e-01 -9.42094177e-02  9.02730078e-02\n",
      " -1.98675245e-01  3.22468020e-02 -4.22513157e-01 -1.01513542e-01\n",
      "  1.04099557e-01 -2.36585543e-01  1.78225487e-01 -1.21879496e-01\n",
      "  1.98791564e-01 -2.77300805e-01  2.91333228e-01 -1.60437614e-01\n",
      " -1.03365280e-01 -2.38217771e-01 -8.74060467e-02  1.17918357e-01\n",
      "  6.23091795e-02  8.25336110e-03  5.44182807e-02 -2.06720367e-01\n",
      " -1.38412982e-01  3.46360624e-01 -9.96583030e-02  3.28844965e-01\n",
      "  1.42124772e-01 -3.15592848e-02  3.00972790e-01  1.73621457e-02\n",
      "  2.12500930e-01  1.43602759e-01  1.71818528e-02  1.51198423e-02\n",
      "  8.48782659e-02  4.61562350e-03 -1.49530187e-01  2.27833316e-01\n",
      " -8.44580755e-02  1.01157874e-01 -2.52896100e-01 -1.50145769e-01\n",
      " -3.90410364e-01  1.01244040e-02 -8.20528939e-02 -4.28183489e-02\n",
      "  1.86476186e-02 -3.74852084e-02 -2.89775252e-01 -2.37249374e-01\n",
      "  8.34000558e-02  2.39327140e-02 -1.36894152e-01  3.78108293e-01\n",
      "  2.07308561e-01 -1.92182422e-01 -1.61796078e-01 -1.34621605e-01\n",
      " -1.61855996e-01 -7.37060967e-04 -2.18305349e-01  9.61575955e-02\n",
      " -1.41028360e-01  1.79092050e-01  1.33396223e-01 -5.02494983e-02\n",
      "  2.29671538e-01  6.67250529e-02 -7.29393065e-02 -1.02746785e-01\n",
      " -1.30528137e-01  1.64245620e-01  1.22920543e-01  2.14730963e-01\n",
      "  2.98712403e-01 -1.97520219e-02 -2.53214002e-01  2.40544870e-01\n",
      " -1.09193422e-01  6.29547387e-02 -3.90767865e-03 -1.63210601e-01\n",
      "  1.28343627e-01 -3.04193646e-01 -5.89342639e-02 -2.04481333e-01\n",
      " -4.35107760e-02  2.06343666e-01 -2.70499825e-01 -1.86332136e-01\n",
      "  1.10883556e-01 -4.50068146e-01  4.74313766e-01  5.51198013e-02\n",
      " -3.47707987e-01 -1.05230428e-01 -5.59277693e-03 -9.44838300e-03\n",
      "  9.05375332e-02  6.88559413e-02 -7.33122677e-02 -1.06939100e-01\n",
      "  2.20463380e-01 -4.64254767e-01  1.53607368e-01  1.00622371e-01\n",
      " -9.06540677e-02  3.08030024e-02 -1.20528415e-01  1.29556641e-01\n",
      " -1.65758301e-02 -3.66749316e-01 -2.24294648e-01 -1.49552360e-01\n",
      "  5.55434078e-02  7.71620423e-02 -4.67143118e-01  2.78803110e-01\n",
      "  4.83863652e-02  1.16567515e-01 -5.18326797e-02  1.44608647e-01\n",
      "  3.24770093e-01  1.69264525e-01 -2.15707743e-03 -2.18896091e-01\n",
      "  8.09373260e-02 -1.75969064e-01 -2.79086769e-01 -7.98893645e-02\n",
      " -2.54421327e-02 -7.56032467e-02  1.41087845e-02  1.18101016e-01\n",
      "  2.06533059e-01 -1.61570534e-01  2.52134167e-02  1.83816954e-01\n",
      "  1.16320185e-01  6.41285479e-02  3.10618758e-01 -4.08361644e-01\n",
      " -1.01062022e-01 -1.06085874e-01  2.41030917e-01 -1.16940103e-01\n",
      "  3.70446302e-04  5.11225872e-02 -6.71000779e-02 -1.02121249e-01\n",
      "  1.31067395e-01 -2.17360482e-01 -4.64699641e-02  1.44026026e-01\n",
      "  7.06754997e-02  1.42340273e-01  2.06489846e-01 -1.29342720e-01\n",
      "  1.97805241e-01 -2.24541277e-01 -1.35177851e-01  1.47257462e-01\n",
      "  2.65910208e-01  3.14068049e-02 -1.58438310e-01  1.09107539e-01\n",
      " -1.44896731e-02 -2.43798271e-01 -1.18807375e-01  2.18357831e-01\n",
      "  3.38003337e-02 -2.00124353e-01 -3.03049892e-01 -3.22075933e-02\n",
      " -3.21582645e-01  2.68655866e-01 -1.23878755e-01 -3.88690799e-01\n",
      " -1.14326455e-01  2.45940350e-02  1.77366108e-01 -1.70190260e-01\n",
      " -6.70747235e-02 -1.80974364e-01  2.88377494e-01 -1.21208079e-01]\n",
      "Words most similar to 'queen': [('executed', 0.29078686237335205), ('try', 0.2864941954612732), ('trouble', 0.28443270921707153), ('business', 0.278394490480423), ('exactly', 0.27413225173950195), ('written', 0.27324673533439636), ('mine', 0.2722795009613037), ('chin', 0.27151837944984436), ('nobody', 0.2705708146095276), ('walk', 0.2674044072628021)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load the text of \"Alice's Adventures in Wonderland\"\n",
    "with open(\"alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    alice_text = file.read()\n",
    "\n",
    "# Preprocess the text: tokenize sentences, remove stop words, and lemmatize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = sent_tokenize(alice_text)\n",
    "tokenized_sentences = [\n",
    "    [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(sentence) if word.isalnum() and word.lower() not in stop_words]\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "# Train Word2Vec model with improved parameters\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=200,  # Increased vector size\n",
    "    window=15,        # Larger context window\n",
    "    min_count=5,      # Ignore rare words\n",
    "    workers=4,\n",
    "    sg=1,             # Use Skip-Gram model\n",
    "    negative=10,      # Negative sampling\n",
    "    epochs=10000         # Train for more epochs\n",
    ")\n",
    "\n",
    "# Save the model for later use\n",
    "word2vec_model.save(\"alice_word2vec_optimized.model\")\n",
    "\n",
    "# Example: Get the word embedding for \"alice\"\n",
    "if \"alice\" in word2vec_model.wv:\n",
    "    print(\"Word embedding for 'alice':\", word2vec_model.wv[\"alice\"])\n",
    "\n",
    "# Example: Find words most similar to \"queen\"\n",
    "if \"queen\" in word2vec_model.wv:\n",
    "    print(\"Words most similar to 'queen':\", word2vec_model.wv.most_similar(\"world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'queen': 0.061967842280864716\n"
     ]
    }
   ],
   "source": [
    "# Example: Calculate cosine similarity between two words\n",
    "try:\n",
    "    word1 = \"alice\"\n",
    "    word2 = \"queen\"\n",
    "    cosine_similarity = word2vec_model.wv.similarity(word1, word2)\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {cosine_similarity}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}. One of the words is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 0.25662288069725037),\n",
       " ('executed', 0.2416895627975464),\n",
       " ('join', 0.21482664346694946),\n",
       " ('asking', 0.20554569363594055),\n",
       " ('fetch', 0.1829044669866562),\n",
       " ('unimportant', 0.18088111281394958),\n",
       " ('evening', 0.17958509922027588),\n",
       " ('repeat', 0.17934490740299225),\n",
       " ('oop', 0.17799676954746246),\n",
       " ('would', 0.17165294289588928)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(\"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.99473584\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9594937\n",
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.8946934\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.8759582\n"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action='ignore')\n",
    " \n",
    " \n",
    "#  Reads ‘alice.txt’ file\n",
    "sample = open(\"alice_in_wonderland.txt\")\n",
    "s = sample.read()\n",
    " \n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    " \n",
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    " \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    " \n",
    "    data.append(temp)\n",
    " \n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count=1,\n",
    "                                vector_size=100, window=5)\n",
    " \n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "      \"and 'wonderland' - CBOW : \",\n",
    "      model1.wv.similarity('alice', 'wonderland'))\n",
    " \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "      \"and 'machines' - CBOW : \",\n",
    "      model1.wv.similarity('alice', 'machines'))\n",
    " \n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count=1, vector_size=100,\n",
    "                                window=5, sg=1)\n",
    " \n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "      \"and 'wonderland' - Skip Gram : \",\n",
    "      model2.wv.similarity('alice', 'wonderland'))\n",
    " \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "      \"and 'machines' - Skip Gram : \",\n",
    "      model2.wv.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97924376"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thought', 0.9901615977287292),\n",
       " (',', 0.9874187111854553),\n",
       " ('herself', 0.9844238758087158),\n",
       " (';', 0.982392430305481),\n",
       " ('so', 0.979295551776886),\n",
       " ('it', 0.9789102077484131),\n",
       " ('hatter', 0.9743627905845642),\n",
       " ('then', 0.9742511510848999),\n",
       " ('but', 0.9740870594978333),\n",
       " ('very', 0.9684762358665466)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar(\"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hard', 0.9070873856544495),\n",
       " ('fall', 0.8905317187309265),\n",
       " ('work', 0.883845329284668),\n",
       " ('get', 0.882828950881958),\n",
       " ('quick', 0.8828085064888),\n",
       " ('easy', 0.8759028911590576),\n",
       " ('yet', 0.8758782744407654),\n",
       " ('clean', 0.8740941882133484),\n",
       " ('working', 0.8733303546905518),\n",
       " ('out', 0.8721160888671875)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('singer', 0.9319669604301453),\n",
       " ('comedian', 0.8841865658760071),\n",
       " ('bollywood', 0.8836835622787476),\n",
       " ('superstar', 0.8688210844993591),\n",
       " ('musician', 0.863633930683136),\n",
       " ('poet', 0.8499681949615479),\n",
       " ('vocalist', 0.8467605710029602),\n",
       " ('entertainer', 0.8449896574020386),\n",
       " ('actor', 0.8414555788040161),\n",
       " ('celebrity', 0.8391618132591248)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('actress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1]\n",
      "X_test Text: ['Congratulations, you won!']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataset\n",
    "emails = [\"Win a free iPhone now\", \"Meeting scheduled at 10 AM\", \"Congratulations, you won!\", \"Please find the report attached\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Spam, 0 = Not Spam\n",
    "\n",
    "# Convert text to vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Train classifier\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "    X, labels, range(len(emails)), test_size=0.25, random_state=67\n",
    ")\n",
    "\n",
    "# Get the original text for X_test\n",
    "X_test_text = [emails[i] for i in test_indices]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"X_test Text:\", X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample dataset\n",
    "reviews = [\"The movie was absolutely fantastic\", \"The plot was terrible and boring\", \"Loved the cinematography\", \"Worst film ever\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Convert text to vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Train classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X, labels)\n",
    "\n",
    "# Predict on new data\n",
    "print(classifier.predict(vectorizer.transform([\"The film was worst\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature Extraction | Captures Meaning? | Handles Rare Words? | Works for Large Text? | Best For                          |\n",
    "|---------------------|-------------------|---------------------|-----------------------|-----------------------------------|\n",
    "| Count Vectorizer    | ❌ No            | ❌ No              | ✅ Yes               | Simple text classification       |\n",
    "| TF-IDF              | ❌ No            | ✅ Yes             | ✅ Yes               | News classification, document ranking |\n",
    "| Word2Vec            | ✅ Yes           | ✅ Yes             | ❌ Needs more data   | Sentiment analysis, chatbots     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample dataset\n",
    "sentences = [\n",
    "    [\"government\", \"passes\", \"economic\", \"reforms\"],\n",
    "    [\"football\", \"team\", \"wins\", \"championship\"],\n",
    "    [\"new\", \"phone\", \"features\", \"AI\", \"camera\"],\n",
    "]\n",
    "labels = [0, 1, 2]  # 0 = Politics, 1 = Sports, 2 = Technology\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Convert sentences to vectorized form (mean of word embeddings)\n",
    "def get_sentence_embedding(sentence):\n",
    "    return np.mean([w2v_model.wv[word] for word in sentence if word in w2v_model.wv], axis=0)\n",
    "\n",
    "X = np.array([get_sentence_embedding(sent) for sent in sentences])\n",
    "\n",
    "# Train classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X, labels)\n",
    "\n",
    "# Predict on new article\n",
    "new_article = [\"team\", \"scores\", \"goal\"]\n",
    "new_embedding = get_sentence_embedding(new_article)\n",
    "print(classifier.predict([new_embedding]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidently-ai3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
