{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional programming, we write explicit rules using constructs like if/else to make decisions. For example, you might write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot\n"
     ]
    }
   ],
   "source": [
    "def classify_temperature(temp):\n",
    "    if temp > 30:\n",
    "        return \"Hot\"\n",
    "    elif temp < 10:\n",
    "        return \"Cold\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "print(classify_temperature(35))  # \"Hot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we manually define thresholds and conditions that determine the outcome. This works well for problems where rules are simple and well understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From If/Else to Machine Learning\n",
    "Machine Learning (ML) represents a shift from explicitly programmed rules to systems that learn decision boundaries from data. Instead of hard-coding rules, you provide examples and let the algorithm find patterns.\n",
    "\n",
    "#### Key Differences:\n",
    "**Deterministic Rules vs. Learned Patterns:**\n",
    "\n",
    "**If/Else:** The decision is fixed by the programmer.\n",
    "\n",
    "**ML:** The model infers a decision boundary from data (which might be non-linear and complex).\n",
    "\n",
    "**Flexibility:**\n",
    "\n",
    "**If/Else:** Adding more conditions often means manually updating rules.\n",
    "\n",
    "**ML:** More data can be used to update or retrain the model, and it can adapt to complex patterns.\n",
    "\n",
    "**Handling Complexity:**\n",
    "\n",
    "**If/Else:** Becomes cumbersome when the number of conditions or features increases.\n",
    "\n",
    "**ML:** Algorithms (e.g., decision trees, neural networks) automatically learn how to weight and combine multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Simple Example: Predicting Categories**\n",
    "\n",
    "**Using If/Else:**\n",
    "Imagine we want to classify fruits as \"Apple\" or \"Orange\" based on features like weight and texture. With if/else, you might have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "def classify_fruit(weight, texture):\n",
    "    if weight < 150 and texture == \"smooth\":\n",
    "        return \"Apple\"\n",
    "    else:\n",
    "        return \"Orange\"\n",
    "\n",
    "print(classify_fruit(120, \"smooth\"))  # \"Apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you assume that all apples are lighter and smooth, and anything else is an orange. However, real-world data might not be that simple.\n",
    "\n",
    "**Using Machine Learning (Decision Tree):**\n",
    "\n",
    "With machine learning, you would:\n",
    "\n",
    "**1.Collect Data:** Gather examples of fruits with labels (Apple, Orange) along with their features.\n",
    "\n",
    "**2.Train a Model:** Let a decision tree or another classifier learn the decision boundaries from the data.\n",
    "\n",
    "**3.Predict:** Use the trained model to classify new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted fruit: Orange\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Example dataset: [weight (grams), texture (0 for smooth, 1 for bumpy)]\n",
    "X = np.array([\n",
    "    [130, 0],  # Orange\n",
    "    [150, 0],  # Orange\n",
    "    [170, 1],  # Apple\n",
    "    [160, 1]   # Apple\n",
    "])\n",
    "y = np.array([\"Orange\", \"Orange\", \"Apple\", \"Apple\"]) \n",
    "\n",
    "# Create and train the decision tree model\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict on a new fruit\n",
    "new_fruit = np.array([[140, 0]])\n",
    "print(\"Predicted fruit:\", clf.predict(new_fruit)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the decision tree automatically figures out the best thresholds on weight and texture. It might learn a rule such as \"if weight > 155 and texture is smooth then Apple, otherwise Orange.\" Notice that the rules aren’t written by a programmer but are derived from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Evolution in a Nutshell:**</u>\n",
    "\n",
    "<u>**If/Else:**</u>\n",
    "\n",
    "**1. Explicit:** You define all the conditions.\n",
    "\n",
    "**2. Limited to simple scenarios:** Can get very complex when rules increase.\n",
    "\n",
    "**3. Static:** Rules don't change unless manually updated.\n",
    "\n",
    "<u>**Machine Learning:**</u>\n",
    "\n",
    "**1. Implicit:** The algorithm learns rules from the data.\n",
    "\n",
    "**2. Handles complexity:** Can model non-linear and high-dimensional decision boundaries.\n",
    "\n",
    "**3. Dynamic:** Models can be retrained as new data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Fruit Classification with Five Features_**\n",
    "\n",
    "Suppose we have the following features for each fruit:\n",
    "\n",
    "1. **Weight (grams)**\n",
    "2. **Color Score** (a numeric value representing redness/brightness)\n",
    "3. **Texture Score** (smoothness vs. roughness, scaled 0–1)\n",
    "4. **Diameter (cm)**\n",
    "5. **Sugar Level** (grams per 100g)\n",
    "\n",
    "We build a dataset with these five features and let a decision tree learn how to classify the fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted fruit: Apple\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset:\n",
    "# Each row corresponds to a fruit sample with five features:\n",
    "# [weight, color score, texture score, diameter, sugar level]\n",
    "X = np.array([\n",
    "    [165, 0.8, 0.2, 7.0, 14],   # Likely an Apple\n",
    "    [160, 0.75, 0.25, 7.2, 15],  # Likely an Apple\n",
    "    [140, 0.6, 0.7, 8.0, 20],    # Likely an Orange\n",
    "    [130, 0.65, 0.6, 8.5, 22],   # Likely an Orange\n",
    "    [170, 0.85, 0.1, 6.5, 13],   # Likely an Apple\n",
    "    [145, 0.55, 0.8, 8.8, 23]    # Likely an Orange\n",
    "])\n",
    "\n",
    "# Labels for the samples\n",
    "y = np.array([\"Apple\", \"Apple\", \"Orange\", \"Orange\", \"Apple\", \"Orange\"])\n",
    "\n",
    "# Create and train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=76)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict on a new fruit sample with five features\n",
    "# New fruit: [weight, color score, texture score, diameter, sugar level]\n",
    "new_fruit = np.array([[155, 0.8, 0.2, 7.0, 14]])\n",
    "prediction = clf.predict(new_fruit)\n",
    "\n",
    "print(\"Predicted fruit:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Fruit 1: Apple\n",
      "Sample Fruit 2: Orange\n"
     ]
    }
   ],
   "source": [
    "def classify_fruit(weight, color_score, texture_score, diameter, sugar_level):\n",
    "    \"\"\"\n",
    "    Classify a fruit as 'Apple' or 'Orange' based on five features.\n",
    "    \n",
    "    Parameters:\n",
    "    - weight: Weight in grams.\n",
    "    - color_score: Numeric score indicating color intensity (higher is redder/brighter).\n",
    "    - texture_score: Score representing smoothness (lower means smoother).\n",
    "    - diameter: Diameter in centimeters.\n",
    "    - sugar_level: Sugar content in grams per 100g.\n",
    "    \n",
    "    Returns:\n",
    "    - A string: \"Apple\" or \"Orange\"\n",
    "    \"\"\"\n",
    "    if (weight > 155 and \n",
    "        color_score > 0.7 and \n",
    "        texture_score < 0.4 and \n",
    "        diameter < 7.5 and \n",
    "        sugar_level < 16):\n",
    "        return \"Apple\"\n",
    "    else:\n",
    "        return \"Orange\"\n",
    "\n",
    "# Test cases:\n",
    "sample_fruit1 = classify_fruit(170, 0.8, 0.2, 7.0, 14)  # Expected: \"Apple\"\n",
    "sample_fruit2 = classify_fruit(150, 0.6, 0.7, 8.0, 20)  # Expected: \"Orange\"\n",
    "\n",
    "print(\"Sample Fruit 1:\", sample_fruit1)\n",
    "print(\"Sample Fruit 2:\", sample_fruit2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Explanation:_**\n",
    "\n",
    "<u>**Manual If/Else Approach:**</u>  \n",
    "To mimic this using if/else, you would have to hard-code conditions for each of the five features (e.g., if weight < threshold and color score > threshold …, etc.). This becomes complex and brittle as the number of features increases.\n",
    "\n",
    "<u>**Machine Learning Approach:**</u>  \n",
    "The decision tree automatically learns the optimal thresholds and splits in the five-dimensional feature space from the provided training data. It then uses these learned rules to classify new fruit samples without manually defined if/else conditions.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Flexibility:** The decision tree can capture non-linear boundaries between classes.\n",
    "\n",
    "2. **Scalability:** As the number of features increases, manually creating if/else rules becomes impractical, whereas ML models handle many features efficiently.\n",
    "\n",
    "3. **Adaptability:** The model can be retrained with new data to improve accuracy without re-writing the rules.\n",
    "\n",
    "This example demonstrates how machine learning evolves the basic if/else paradigm into a data-driven approach that can handle multiple features and complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Apple       1.00      1.00      1.00        23\n",
      "      Orange       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Prediction for apple sample: Apple\n",
      "Prediction for the orange sample: Orange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Generate synthetic data for 100 apples\n",
    "apples = np.column_stack([\n",
    "    np.random.normal(170, 5, 100),       # Weight around 170 grams\n",
    "    np.random.normal(0.8, 0.05, 100),    # Color Score high\n",
    "    np.random.normal(0.2, 0.05, 100),    # Texture Score low\n",
    "    np.random.normal(7.0, 0.2, 100),     # Diameter around 7.0 cm\n",
    "    np.random.normal(14, 1, 100)         # Sugar Level lower\n",
    "])\n",
    "\n",
    "# Generate synthetic data for 100 oranges\n",
    "oranges = np.column_stack([\n",
    "    np.random.normal(150, 5, 100),       # Weight around 150 grams\n",
    "    np.random.normal(0.6, 0.05, 100),    # Color Score lower\n",
    "    np.random.normal(0.7, 0.05, 100),    # Texture Score higher\n",
    "    np.random.normal(8.0, 0.2, 100),     # Diameter around 8.0 cm\n",
    "    np.random.normal(20, 1, 100)         # Sugar Level higher\n",
    "])\n",
    "\n",
    "# Combine the data and create labels\n",
    "X = np.vstack([apples, oranges])\n",
    "y = np.array([\"Apple\"] * 100 + [\"Orange\"] * 100)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=76, shuffle=True)\n",
    "\n",
    "# Create and train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=76)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on a new fruit sample\n",
    "# New fruit features: [weight, color score, texture score, diameter, sugar level]\n",
    "apple_sample = np.array([[170, 0.82, 0.22, 7.1, 14]])\n",
    "apple_sample_scaled = scaler.transform(apple_sample)  # Scale the new sample\n",
    "print(\"Prediction for apple sample:\", clf.predict(apple_sample_scaled)[0])\n",
    "\n",
    "# Sample features for an orange: [weight, color score, texture score, diameter, sugar level]\n",
    "orange_sample = np.array([[155, 0.65, 0.75, 8.2, 19]])  # Example values for an orange\n",
    "\n",
    "# Scale the sample using the same scaler used for training\n",
    "orange_sample_scaled = scaler.transform(orange_sample)\n",
    "\n",
    "# Predict the class of the sample\n",
    "orange_prediction = clf.predict(orange_sample_scaled)\n",
    "print(\"Prediction for the orange sample:\", orange_prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidently-ai3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
